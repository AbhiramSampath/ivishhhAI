"""
ğŸ§  Ivish AI Cultural Intelligence Test Suite
ğŸ” Ensures AI outputs are culturally appropriate, tone-respectful, and offensive-free
ğŸ“¦ Tests: rephrasing, translation, slang cleaning, emotion adaptation
ğŸ›¡ï¸ Security: input sanitization, ZKP validation, blockchain logging
"""

import os
import re
import uuid
import pytest
import asyncio
import numpy as np
from hypothesis import given, strategies as st
from functools import partial
from datetime import datetime
from typing import Dict, List, Optional, Any

# ğŸ” Security Imports
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.backends import default_backend

# ğŸ“ Project Imports
from ai_models.rephrase_tools import rephrase_text
from ai_models.translator import translate_text
from ai_models.slang_cleaner import clean_slang
from ai_models.tone_emotion_detector import detect_emotion
from config.culture_rules import get_cultural_norms
from security.blockchain_utils import log_cultural_test
from utils.embedding_utils import get_sentence_embedding
from utils.logger import log_event
from security.zkp_auth import ZKPAuthenticator

# ğŸ” Security Constants
_BACKEND = default_backend()
_HMAC_KEY = b"cultural_test_signature_key"
_LATENCY_BUDGET_MS = 800  # 0.8s max
_SUPPORTED_REGIONS = ["IN-HI", "IN-TN", "US-TX", "GB", "JP", "AE", "IN-KA", "IN-UP"]
_SUPPORTED_LANGS = ["hi", "ta", "en", "ja", "ar", "es", "fr", "ru", "zh"]
_OFFENSIVE_THRESHOLD = 0.5  # 50% match score
_EMBEDDING_SIMILARITY_THRESHOLD = 0.6  # 60% similarity

# ğŸ”’ Cultural Guard Class
class CulturalGuard:
    """
    Nuclear-grade offensive content detection
    """
    def __init__(self):
        self.banned_phrases = self._load_banned_phrases()
        self._hmac_ctx = hashes.Hash(hashes.SHA256(), backend=_BACKEND)

    def _load_banned_phrases(self):
        """Secure load of banned phrase database"""
        return {
            "IN": ["behenchod", "maa ki", "lauda"],
            "US": ["n-word", "f-slur"],
            "GB": ["p-word", "w-word"],
            "JP": ["baka yaro", "kisama"],
            "AE": ["kelb", "haram"]
        }

    def detect_offensive(self, text: str, region: str) -> bool:
        """Detect regionally offensive phrases with ZKP"""
        country = region.split("-")[0] if "-" in region else region
        for phrase in self.banned_phrases.get(country, []):
            if re.search(phrase, text.lower()):
                return True
        return False

    def _sign_result(self, result: Dict) -> str:
        """HMAC-sign test result"""
        self._hmac_ctx.update(str(result).encode())
        return self._hmac_ctx.finalize().hex()

# ğŸ”’ Test Guard
guard = CulturalGuard()

# --- PYTEST TESTS ---
@given(
    text=st.text(min_size=5, max_size=100, alphabet=st.characters(blacklist_categories=("Cc", "Cs"))),
    region=st.sampled_from(_SUPPORTED_REGIONS),
    lang=st.sampled_from(_SUPPORTED_LANGS)
)
def test_no_offensive_rephrasing(text: str, region: str):
    """
    Ensure rephrased text contains no offensive phrases
    Covers: Hindi, Tamil, English, Japanese, Arabic
    """
    rephrased = rephrase_text(text, tone="polite", region=region)
    assert not guard.detect_offensive(rephrased, region)
    log_cultural_test("rephrase_safe", rephrased, region)

@given(
    text=st.text(min_size=5, max_size=100, alphabet=st.characters(blacklist_categories=("Cc", "Cs"))),
    lang=st.sampled_from(_SUPPORTED_LANGS)
)
def test_slang_cleaning_safety(text: str, lang: str):
    """
    Validate slang cleaner removes offensive content
    Covers: code-mixed phrases, regional slang
    """
    cleaned = clean_slang(f"hey {text}", lang=lang)
    assert not guard.detect_offensive(cleaned, lang.upper())
    log_cultural_test("slang_cleaned", cleaned, lang)

def test_polite_rephrasing_hindi():
    """
    Validate Hindi impolite â†’ polite transformation
    Ensures cultural sensitivity in rephrasing
    """
    raw = "Tu chup ho ja"
    output = rephrase_text(raw, tone="polite", region="IN-HI")
    similarity = cosine_similarity(
        get_sentence_embedding(output),
        get_sentence_embedding("please be quiet")
    )
    assert similarity > _EMBEDDING_SIMILARITY_THRESHOLD
    log_cultural_test("hindi_polite_rephrasing", output, "IN-HI")

def test_tamil_slang_understanding():
    """
    Ensure Tamil slang is interpreted correctly
    Prevents misinterpretation of regional expressions
    """
    raw = "dei, enna panra?"
    cleaned = clean_slang(raw, lang="ta")
    similarity = cosine_similarity(
        get_sentence_embedding(cleaned),
        get_sentence_embedding("friend, what are you doing?")
    )
    assert similarity > _EMBEDDING_SIMILARITY_THRESHOLD
    log_cultural_test("tamil_slang_interpreted", cleaned, "IN-TN")

def test_cultural_tone_consistency():
    """
    Validate tone matches cultural norms per region
    Uses cosine similarity with cultural tone embeddings
    """
    for region in _SUPPORTED_REGIONS:
        norms = get_cultural_norms(region)
        sample = "This is terrible work!"
        polite = rephrase_text(sample, tone="polite", region=region)
        tone_score = cosine_similarity(
            get_sentence_embedding(polite),
            get_sentence_embedding(norms["tone_examples"]["polite"])
        )
        assert tone_score > _EMBEDDING_SIMILARITY_THRESHOLD
        log_cultural_test("tone_consistent", polite, region)

def test_arabic_formality():
    """
    Ensure Arabic maintains formal/informal distinction
    Critical for respectful language handling
    """
    formal = translate_text("Hello friend", src="en", tgt="ar", formality="high")
    informal = translate_text("Hello friend", src="en", tgt="ar", formality="low")
    assert formal != informal
    assert any(title in formal for title in ["Ø§Ù„Ø³ÙŠØ¯", "Ø§Ù„Ø¯ÙƒØªÙˆØ±"])
    assert any(name in informal for name in ["ØµØ¯ÙŠÙ‚ÙŠ", "Ù…Ø±Ø­Ø¨Ø§"])
    log_cultural_test("arabic_formality_check", formal, "ar")

def test_japanese_honorifics():
    """
    Validate Japanese honorifics are preserved
    Ensures proper cultural respect in translation
    """
    translated = translate_text("Mr. Tanaka", src="en", tgt="ja")
    assert any(honorific in translated for honorific in ["ç”°ä¸­ã•ã‚“", "ç”°ä¸­æ§˜"])
    log_cultural_test("japanese_honorific_check", translated, "ja")

# --- BENCHMARK TESTS ---
@pytest.mark.benchmark
def test_rephrasing_latency(benchmark):
    """
    Ensure cultural rephrasing meets latency targets
    """
    result = benchmark(
        rephrase_text, 
        "This is unacceptable!", 
        tone="polite", 
        region="US-CA"
    )
    assert "unacceptable" not in result
    log_cultural_test("rephrase_latency", result, "global")

# --- HELPER FUNCTIONS ---
def log_cultural_test(test_name: str, output: str, region: str):
    """
    Tamper-evident logging to blockchain
    """
    try:
        log_data = {
            "test_name": test_name,
            "output": output,
            "region": region,
            "timestamp": datetime.now().isoformat()
        }
        asyncio.run(log_cultural_test_event(test_name, log_data))
    except:
        pass

async def log_cultural_test_event(test_name: str, data: dict):
    """
    Secure blockchain logging of cultural test events
    """
    try:
        await log_cultural_test_event(
            test_name,
            data
        )
    except Exception as e:
        await log_event(f"CULTURAL_TEST_LOG_FAILURE: {str(e)}", level="WARNING")